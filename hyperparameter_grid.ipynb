{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pretrain dataset text creation \n",
    "This commands will convert the csv input file into `.txt` files ready to be used in pre-train stages. You can create a single file or a version already splitted into train and text partition (use the `--split` command)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "nsp_data_folder = 'data/nsp'\n",
    "mlm_only_data_folder = 'data/mlm_only'\n",
    "finetune_data_folder = 'data/finetune'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nsp_only and nsp_mlm dataset\n",
    "!python3 bert_medical_records/preprocessing_python/text_generator.py \\\n",
    "    --file_path bert_medical_records/data/trajectories_training_set.csv \\\n",
    "    --output_folder {nsp_data_folder} \\\n",
    "    --create_nsp_class_text_data \\\n",
    "    --split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mlm_only dataset\n",
    "!python3 bert_medical_records/preprocessing_python/text_generator.py \\\n",
    "    --file_path bert_medical_records/data/trajectories_training_set.csv \\\n",
    "    --output_folder {mlm_only_data_folder} \\\n",
    "    --create_mlm_only_dataset \\\n",
    "    --split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# finetuning dataset\n",
    "!python3 bert_medical_records/preprocessing_python/text_generator.py \\\n",
    "    --file_path bert_medical_records/data/trajectories_training_set.csv \\\n",
    "    --output_folder {finetune_data_folder} \\\n",
    "    --create_finetuning_text_data \\\n",
    "    --split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# COMPLETE GRID\n",
    "#\n",
    "# This hyperparameter grid is about training parameters. It doesn't change the model architecture \n",
    "grid = {\n",
    "    'already_pretrained' : [True, False],\n",
    "    'pre_train_tasks': ['mlm_nsp', 'nsp', 'mlm'],\n",
    "    'learning_rate' : [1e-5, 3e-5, 5e-5],\n",
    "    'warmup_step' : [500, 1_000],\n",
    "    'max_seq_length' : [128,256,512],\n",
    "    'type_of_scheduler' : ['constant_with_warmup', 'linear'],\n",
    "    'batch_size' : [16,32],\n",
    "    'num_epochs' : [16, 32],\n",
    "    'hidden_size' : [512, 1024],\n",
    "    'num_hidden_layers' : [18,24],\n",
    "    'num_attention_heads' : [8,16],\n",
    "    'intermediate_size' : [2048, 4096]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training grid\n",
    "Here there are some parameters that will change some parameters that are needed for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRAINING GRID\n",
    "#\n",
    "# This hyperparameter grid is about training parameters. It doesn't change the model architecture \n",
    "grid = {\n",
    "    'pre_train_tasks': ['mlm_nsp', 'nsp'],\n",
    "    'learning_rate' : [3e-5, 5e-5],\n",
    "    'warmup_step' : [1_000],\n",
    "    'max_seq_length' : [128,512],\n",
    "    'type_of_scheduler' : ['constant_with_warmup', 'linear'],\n",
    "    'batch_size' : [16,32],\n",
    "    'num_epochs' : [16, 32]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run PRETRAIN & FINETUNING\n",
    "This set of commands will run pre_train and finetuning of the bert model on the selected dataset. The finetuning will serve as evaluation of the performance of the pretrained model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NSP_&_MLM\n",
    "- `batch_size = 16`\n",
    "- `num_epochs = 16`\n",
    "- `learning_rate = 3e-5`\n",
    "- `scheduler = constant_with_warmup && linear`\n",
    "- `max_seq_length = 128`\n",
    "- `use_pretrained = False && True`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_time = datetime.now().strftime(\"%d-%m-%Y_%H-%M\")\n",
    "!python3 run_pre_train.py \\\n",
    "    --do_train \\\n",
    "    --do_eval \\\n",
    "    --train_batch_size 16 \\\n",
    "    --input_file {nsp_data_folder} \\\n",
    "    --num_epochs 16 \\\n",
    "    --pre_train_tasks mlm_nsp \\\n",
    "    --learning_rate 3e-5 \\\n",
    "    --scheduler_name constant_with_warmup \\\n",
    "    --num_warmup_step 1000 \\\n",
    "    --max_seq_length 128 \\\n",
    "    --output_dir output/{current_time}\n",
    "!python3 run_glue.py \\\n",
    "    --input_file {finetune_data_folder} \\\n",
    "    --model_input output/{current_time} \\\n",
    "    --do_train \\\n",
    "    --do_eval \\\n",
    "    --num_epochs 8 \\\n",
    "    --train_batch_size 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_time = datetime.now().strftime(\"%d-%m-%Y_%H-%M\")\n",
    "!python3 run_pre_train.py \\\n",
    "    --do_train \\\n",
    "    --do_eval \\\n",
    "    --train_batch_size 16 \\\n",
    "    --input_file {nsp_data_folder} \\\n",
    "    --num_epochs 16 \\\n",
    "    --pre_train_tasks mlm_nsp \\\n",
    "    --learning_rate 3e-5 \\\n",
    "    --scheduler_name linear \\\n",
    "    --num_warmup_step 1000 \\\n",
    "    --max_seq_length 128 \\\n",
    "    --output_dir output/{current_time}\n",
    "!python3 run_glue.py \\\n",
    "    --input_file {finetune_data_folder} \\\n",
    "    --model_input output/{current_time} \\\n",
    "    --do_train \\\n",
    "    --do_eval \\\n",
    "    --num_epochs 8 \\\n",
    "    --train_batch_size 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_time = datetime.now().strftime(\"%d-%m-%Y_%H-%M\")\n",
    "!python3 run_pre_train.py \\\n",
    "    --do_train \\\n",
    "    --do_eval \\\n",
    "    --train_batch_size 16 \\\n",
    "    --input_file {nsp_data_folder} \\\n",
    "    --num_epochs 16 \\\n",
    "    --pre_train_tasks mlm_nsp \\\n",
    "    --learning_rate 3e-5 \\\n",
    "    --scheduler_name linear \\\n",
    "    --num_warmup_step 1000 \\\n",
    "    --max_seq_length 128 \\\n",
    "    --use_pretrained_bert \\\n",
    "    --output_dir output/{current_time}\n",
    "!python3 run_glue.py \\\n",
    "    --input_file {finetune_data_folder} \\\n",
    "    --model_input output/{current_time} \\\n",
    "    --do_train \\\n",
    "    --do_eval \\\n",
    "    --num_epochs 8 \\\n",
    "    --train_batch_size 16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `batch_size = 32`\n",
    "- `num_epochs = 32`\n",
    "- `learning_rate = 5e-5`\n",
    "- `scheduler = constant_with_warmup`\n",
    "- `max_seq_length = 256 && 512`\n",
    "- `use_pretrained = False && True`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_time = datetime.now().strftime(\"%d-%m-%Y_%H-%M\")\n",
    "!python3 run_pre_train.py \\\n",
    "    --do_train \\\n",
    "    --do_eval \\\n",
    "    --train_batch_size 32 \\\n",
    "    --input_file {nsp_data_folder} \\\n",
    "    --num_epochs 32 \\\n",
    "    --pre_train_tasks mlm_nsp \\\n",
    "    --learning_rate 5e-5 \\\n",
    "    --scheduler_name constant_with_warmup \\\n",
    "    --num_warmup_step 1000 \\\n",
    "    --max_seq_length 256 \\\n",
    "    --output_dir output/{current_time}\n",
    "!python3 run_glue.py \\\n",
    "    --input_file {finetune_data_folder} \\\n",
    "    --model_input output/{current_time} \\\n",
    "    --do_train \\\n",
    "    --do_eval \\\n",
    "    --num_epochs 16 \\\n",
    "    --train_batch_size 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_time = datetime.now().strftime(\"%d-%m-%Y_%H-%M\")\n",
    "!python3 run_pre_train.py \\\n",
    "    --do_train \\\n",
    "    --do_eval \\\n",
    "    --train_batch_size 32 \\\n",
    "    --input_file {nsp_data_folder} \\\n",
    "    --num_epochs 32 \\\n",
    "    --pre_train_tasks mlm_nsp \\\n",
    "    --learning_rate 5e-5 \\\n",
    "    --scheduler_name constant_with_warmup \\\n",
    "    --num_warmup_step 1000 \\\n",
    "    --max_seq_length 256 \\\n",
    "    --use_pretrained_bert \\\n",
    "    --output_dir output/{current_time}\n",
    "!python3 run_glue.py \\\n",
    "    --input_file {finetune_data_folder} \\\n",
    "    --model_input output/{current_time} \\\n",
    "    --do_train \\\n",
    "    --do_eval \\\n",
    "    --num_epochs 16 \\\n",
    "    --train_batch_size 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_time = datetime.now().strftime(\"%d-%m-%Y_%H-%M\")\n",
    "!python3 run_pre_train.py \\\n",
    "    --do_train \\\n",
    "    --do_eval \\\n",
    "    --train_batch_size 32 \\\n",
    "    --input_file {nsp_data_folder} \\\n",
    "    --num_epochs 32 \\\n",
    "    --pre_train_tasks mlm_nsp \\\n",
    "    --learning_rate 5e-5 \\\n",
    "    --scheduler_name constant_with_warmup \\\n",
    "    --num_warmup_step 1000 \\\n",
    "    --max_seq_length 512 \\\n",
    "    --output_dir output/{current_time}\n",
    "!python3 run_glue.py \\\n",
    "    --input_file {finetune_data_folder} \\\n",
    "    --model_input output/{current_time} \\\n",
    "    --do_train \\\n",
    "    --do_eval \\\n",
    "    --num_epochs 16 \\\n",
    "    --train_batch_size 32"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NSP_ONLY\n",
    "- `batch_size = 16`\n",
    "- `num_epochs = 16`\n",
    "- `learning_rate = 3e-5`\n",
    "- `scheduler = constant_with_warmup && linear`\n",
    "- `max_seq_length = 128`\n",
    "- `use_pretrained = False && True`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_time = datetime.now().strftime(\"%d-%m-%Y_%H-%M\")\n",
    "!python3 run_pre_train.py \\\n",
    "    --do_train \\\n",
    "    --do_eval \\\n",
    "    --train_batch_size 16 \\\n",
    "    --input_file {nsp_data_folder} \\\n",
    "    --num_epochs 16 \\\n",
    "    --pre_train_tasks nsp \\\n",
    "    --learning_rate 3e-5 \\\n",
    "    --scheduler_name constant_with_warmup \\\n",
    "    --num_warmup_step 1000 \\\n",
    "    --max_seq_length 128 \\\n",
    "    --output_dir output/{current_time}\n",
    "!python3 run_glue.py \\\n",
    "    --input_file {finetune_data_folder} \\\n",
    "    --model_input output/{current_time} \\\n",
    "    --do_train \\\n",
    "    --do_eval \\\n",
    "    --num_epochs 8 \\\n",
    "    --train_batch_size 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_time = datetime.now().strftime(\"%d-%m-%Y_%H-%M\")\n",
    "!python3 run_pre_train.py \\\n",
    "    --do_train \\\n",
    "    --do_eval \\\n",
    "    --train_batch_size 16 \\\n",
    "    --input_file {nsp_data_folder} \\\n",
    "    --num_epochs 16 \\\n",
    "    --pre_train_tasks nsp \\\n",
    "    --learning_rate 3e-5 \\\n",
    "    --scheduler_name linear \\\n",
    "    --num_warmup_step 1000 \\\n",
    "    --max_seq_length 128 \\\n",
    "    --output_dir output/{current_time}\n",
    "!python3 run_glue.py \\\n",
    "    --input_file {finetune_data_folder} \\\n",
    "    --model_input output/{current_time} \\\n",
    "    --do_train \\\n",
    "    --do_eval \\\n",
    "    --num_epochs 8 \\\n",
    "    --train_batch_size 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_time = datetime.now().strftime(\"%d-%m-%Y_%H-%M\")\n",
    "!python3 run_pre_train.py \\\n",
    "    --do_train \\\n",
    "    --do_eval \\\n",
    "    --train_batch_size 16 \\\n",
    "    --input_file {nsp_data_folder} \\\n",
    "    --num_epochs 16 \\\n",
    "    --pre_train_tasks nsp \\\n",
    "    --learning_rate 3e-5 \\\n",
    "    --scheduler_name linear \\\n",
    "    --num_warmup_step 1000 \\\n",
    "    --max_seq_length 128 \\\n",
    "    --use_pretrained_bert \\\n",
    "    --output_dir output/{current_time}\n",
    "!python3 run_glue.py \\\n",
    "    --input_file {finetune_data_folder} \\\n",
    "    --model_input output/{current_time} \\\n",
    "    --do_train \\\n",
    "    --do_eval \\\n",
    "    --num_epochs 8 \\\n",
    "    --train_batch_size 16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `batch_size = 32`\n",
    "- `num_epochs = 32`\n",
    "- `learning_rate = 5e-5`\n",
    "- `scheduler = constant_with_warmup`\n",
    "- `max_seq_length = 256 && 512`\n",
    "- `use_pretrained = False && True`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_time = datetime.now().strftime(\"%d-%m-%Y_%H-%M\")\n",
    "!python3 run_pre_train.py \\\n",
    "    --do_train \\\n",
    "    --do_eval \\\n",
    "    --train_batch_size 32 \\\n",
    "    --input_file {nsp_data_folder} \\\n",
    "    --num_epochs 32 \\\n",
    "    --pre_train_tasks nsp \\\n",
    "    --learning_rate 5e-5 \\\n",
    "    --scheduler_name constant_with_warmup \\\n",
    "    --num_warmup_step 1000 \\\n",
    "    --max_seq_length 256 \\\n",
    "    --output_dir output/{current_time}\n",
    "!python3 run_glue.py \\\n",
    "    --input_file {finetune_data_folder} \\\n",
    "    --model_input output/{current_time} \\\n",
    "    --do_train \\\n",
    "    --do_eval \\\n",
    "    --num_epochs 16 \\\n",
    "    --train_batch_size 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_time = datetime.now().strftime(\"%d-%m-%Y_%H-%M\")\n",
    "!python3 run_pre_train.py \\\n",
    "    --do_train \\\n",
    "    --do_eval \\\n",
    "    --train_batch_size 32 \\\n",
    "    --input_file {nsp_data_folder} \\\n",
    "    --num_epochs 32 \\\n",
    "    --pre_train_tasks nsp \\\n",
    "    --learning_rate 5e-5 \\\n",
    "    --scheduler_name constant_with_warmup \\\n",
    "    --num_warmup_step 1000 \\\n",
    "    --max_seq_length 256 \\\n",
    "    --use_pretrained_bert \\\n",
    "    --output_dir output/{current_time}\n",
    "!python3 run_glue.py \\\n",
    "    --input_file {finetune_data_folder} \\\n",
    "    --model_input output/{current_time} \\\n",
    "    --do_train \\\n",
    "    --do_eval \\\n",
    "    --num_epochs 16 \\\n",
    "    --train_batch_size 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_time = datetime.now().strftime(\"%d-%m-%Y_%H-%M\")\n",
    "!python3 run_pre_train.py \\\n",
    "    --do_train \\\n",
    "    --do_eval \\\n",
    "    --train_batch_size 32 \\\n",
    "    --input_file {nsp_data_folder} \\\n",
    "    --num_epochs 32 \\\n",
    "    --pre_train_tasks nsp \\\n",
    "    --learning_rate 5e-5 \\\n",
    "    --scheduler_name constant_with_warmup \\\n",
    "    --num_warmup_step 1000 \\\n",
    "    --max_seq_length 512 \\\n",
    "    --output_dir output/{current_time}\n",
    "!python3 run_glue.py \\\n",
    "    --input_file {finetune_data_folder} \\\n",
    "    --model_input output/{current_time} \\\n",
    "    --do_train \\\n",
    "    --do_eval \\\n",
    "    --num_epochs 16 \\\n",
    "    --train_batch_size 32"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MLM_ONLY\n",
    "- `batch_size = 16`\n",
    "- `num_epochs = 16`\n",
    "- `learning_rate = 3e-5`\n",
    "- `scheduler = constant_with_warmup && linear`\n",
    "- `max_seq_length = 128`\n",
    "- `use_pretrained = False && True`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_time = datetime.now().strftime(\"%d-%m-%Y_%H-%M\")\n",
    "!python3 run_pre_train.py \\\n",
    "    --do_train \\\n",
    "    --do_eval \\\n",
    "    --train_batch_size 16 \\\n",
    "    --input_file {mlm_only_data_folder} \\\n",
    "    --num_epochs 16 \\\n",
    "    --pre_train_tasks mlm \\\n",
    "    --learning_rate 3e-5 \\\n",
    "    --scheduler_name constant_with_warmup \\\n",
    "    --num_warmup_step 1000 \\\n",
    "    --max_seq_length 128 \\\n",
    "    --output_dir output/{current_time}\n",
    "!python3 run_glue.py \\\n",
    "    --input_file {finetune_data_folder} \\\n",
    "    --model_input output/{current_time} \\\n",
    "    --do_train \\\n",
    "    --do_eval \\\n",
    "    --num_epochs 8 \\\n",
    "    --train_batch_size 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_time = datetime.now().strftime(\"%d-%m-%Y_%H-%M\")\n",
    "!python3 run_pre_train.py \\\n",
    "    --do_train \\\n",
    "    --do_eval \\\n",
    "    --train_batch_size 16 \\\n",
    "    --input_file {mlm_only_data_folder} \\\n",
    "    --num_epochs 16 \\\n",
    "    --pre_train_tasks mlm \\\n",
    "    --learning_rate 3e-5 \\\n",
    "    --scheduler_name linear \\\n",
    "    --num_warmup_step 1000 \\\n",
    "    --max_seq_length 128 \\\n",
    "    --output_dir output/{current_time}\n",
    "!python3 run_glue.py \\\n",
    "    --input_file {finetune_data_folder} \\\n",
    "    --model_input output/{current_time} \\\n",
    "    --do_train \\\n",
    "    --do_eval \\\n",
    "    --num_epochs 8 \\\n",
    "    --train_batch_size 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_time = datetime.now().strftime(\"%d-%m-%Y_%H-%M\")\n",
    "!python3 run_pre_train.py \\\n",
    "    --do_train \\\n",
    "    --do_eval \\\n",
    "    --train_batch_size 16 \\\n",
    "    --input_file {mlm_only_data_folder} \\\n",
    "    --num_epochs 16 \\\n",
    "    --pre_train_tasks mlm \\\n",
    "    --learning_rate 3e-5 \\\n",
    "    --scheduler_name linear \\\n",
    "    --num_warmup_step 1000 \\\n",
    "    --max_seq_length 128 \\\n",
    "    --use_pretrained_bert \\\n",
    "    --output_dir output/{current_time}\n",
    "!python3 run_glue.py \\\n",
    "    --input_file {finetune_data_folder} \\\n",
    "    --model_input output/{current_time} \\\n",
    "    --do_train \\\n",
    "    --do_eval \\\n",
    "    --num_epochs 8 \\\n",
    "    --train_batch_size 16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `batch_size = 32`\n",
    "- `num_epochs = 32`\n",
    "- `learning_rate = 5e-5`\n",
    "- `scheduler = constant_with_warmup`\n",
    "- `max_seq_length = 256 && 512`\n",
    "- `use_pretrained = False && True`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_time = datetime.now().strftime(\"%d-%m-%Y_%H-%M\")\n",
    "!python3 run_pre_train.py \\\n",
    "    --do_train \\\n",
    "    --do_eval \\\n",
    "    --train_batch_size 32 \\\n",
    "    --input_file {mlm_only_data_folder} \\\n",
    "    --num_epochs 32 \\\n",
    "    --pre_train_tasks mlm_nsp \\\n",
    "    --learning_rate 5e-5 \\\n",
    "    --scheduler_name constant_with_warmup \\\n",
    "    --num_warmup_step 1000 \\\n",
    "    --max_seq_length 256 \\\n",
    "    --output_dir output/{current_time}\n",
    "!python3 run_glue.py \\\n",
    "    --input_file {finetune_data_folder} \\\n",
    "    --model_input output/{current_time} \\\n",
    "    --do_train \\\n",
    "    --do_eval \\\n",
    "    --num_epochs 16 \\\n",
    "    --train_batch_size 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_time = datetime.now().strftime(\"%d-%m-%Y_%H-%M\")\n",
    "!python3 run_pre_train.py \\\n",
    "    --do_train \\\n",
    "    --do_eval \\\n",
    "    --train_batch_size 32 \\\n",
    "    --input_file {mlm_only_data_folder} \\\n",
    "    --num_epochs 32 \\\n",
    "    --pre_train_tasks mlm_nsp \\\n",
    "    --learning_rate 5e-5 \\\n",
    "    --scheduler_name constant_with_warmup \\\n",
    "    --num_warmup_step 1000 \\\n",
    "    --max_seq_length 256 \\\n",
    "    --use_pretrained_bert \\\n",
    "    --output_dir output/{current_time}\n",
    "!python3 run_glue.py \\\n",
    "    --input_file {finetune_data_folder} \\\n",
    "    --model_input output/{current_time} \\\n",
    "    --do_train \\\n",
    "    --do_eval \\\n",
    "    --num_epochs 16 \\\n",
    "    --train_batch_size 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_time = datetime.now().strftime(\"%d-%m-%Y_%H-%M\")\n",
    "!python3 run_pre_train.py \\\n",
    "    --do_train \\\n",
    "    --do_eval \\\n",
    "    --train_batch_size 32 \\\n",
    "    --input_file {mlm_only_data_folder} \\\n",
    "    --num_epochs 32 \\\n",
    "    --pre_train_tasks mlm_nsp \\\n",
    "    --learning_rate 5e-5 \\\n",
    "    --scheduler_name constant_with_warmup \\\n",
    "    --num_warmup_step 1000 \\\n",
    "    --max_seq_length 512 \\\n",
    "    --output_dir output/{current_time}\n",
    "!python3 run_glue.py \\\n",
    "    --input_file {finetune_data_folder} \\\n",
    "    --model_input output/{current_time} \\\n",
    "    --do_train \\\n",
    "    --do_eval \\\n",
    "    --num_epochs 16 \\\n",
    "    --train_batch_size 32"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Architecture grid\n",
    "Here there a grid that change the model architecture, after the first grid search it can be done a second try to improve the best model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ARCHITECTURE GRID\n",
    "#\n",
    "# This hyperparameters change the model architecture, \n",
    "# for an optional second grid search for improving the best model found in the previous grid search\n",
    "grid = {\n",
    "    'hidden_size' : [512, 1024],\n",
    "    'num_hidden_layers' : [18,24],\n",
    "    'num_attention_heads' : [8,16],\n",
    "    'intermediate_size' : [2048, 4096]\n",
    "}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
